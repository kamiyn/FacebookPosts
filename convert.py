#!/usr/bin/env python3
"""
Facebook JSON ãƒ‡ãƒ¼ã‚¿ã‚’ Hugo ãƒ–ãƒ­ã‚°è¨˜äº‹ã«å¤‰æ›ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
import os
import shutil
from datetime import datetime
from pathlib import Path
import re


def decode_facebook_text(text: str) -> str:
    """Facebook JSON ã®ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚ŒãŸ UTF-8 ã‚’æ­£ã—ããƒ‡ã‚³ãƒ¼ãƒ‰"""
    if not text:
        return ""
    try:
        # Facebook ã¯ Latin-1 ã¨ã—ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸ UTF-8 ãƒã‚¤ãƒˆã‚’å‡ºåŠ›ã™ã‚‹
        return text.encode('latin-1').decode('utf-8')
    except (UnicodeDecodeError, UnicodeEncodeError):
        return text


def sanitize_filename(text: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã—ã¦ä½¿ãˆã‚‹æ–‡å­—åˆ—ã«å¤‰æ›"""
    # æ”¹è¡Œã¨ã‚¿ãƒ–ã‚’ç©ºç™½ã«
    text = re.sub(r'[\n\r\t]+', ' ', text)
    # ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ãˆãªã„æ–‡å­—ã‚’é™¤å»
    text = re.sub(r'[<>:"/\\|?*]', '', text)
    # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«
    text = re.sub(r'\s+', ' ', text)
    # å‰å¾Œã®ç©ºç™½ã‚’é™¤å»ã—ã€é•·ã•ã‚’åˆ¶é™
    return text.strip()[:50]


def convert_timestamp(ts: int) -> tuple[str, str]:
    """Unix ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ—¥ä»˜æ–‡å­—åˆ—ã«å¤‰æ›"""
    dt = datetime.fromtimestamp(ts)
    return dt.strftime('%Y-%m-%d'), dt.strftime('%Y-%m-%dT%H:%M:%S+09:00')


def extract_post_content(post: dict) -> str:
    """æŠ•ç¨¿ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º"""
    content = ""
    if 'data' in post:
        for data in post['data']:
            if isinstance(data, dict) and 'post' in data:
                content = decode_facebook_text(data['post'])
                break
    return content


def extract_attachments(post: dict) -> list[dict]:
    """æŠ•ç¨¿ã‹ã‚‰æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’æŠ½å‡º"""
    attachments = []
    if 'attachments' not in post:
        return attachments

    for att in post['attachments']:
        if 'data' not in att:
            continue
        for data in att['data']:
            if 'media' in data:
                media = data['media']
                attachments.append({
                    'type': 'media',
                    'uri': media.get('uri', ''),
                    'description': decode_facebook_text(media.get('description', '')),
                    'title': decode_facebook_text(media.get('title', ''))
                })
            elif 'external_context' in data:
                ext = data['external_context']
                attachments.append({
                    'type': 'link',
                    'url': ext.get('url', ''),
                    'name': decode_facebook_text(ext.get('name', ''))
                })
            elif 'place' in data:
                place = data['place']
                attachments.append({
                    'type': 'place',
                    'name': decode_facebook_text(place.get('name', '')),
                    'address': decode_facebook_text(place.get('address', ''))
                })
    return attachments


def generate_hugo_frontmatter(date_iso: str, title: str, tags: list[str] = None) -> str:
    """Hugo ã®ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã‚’ç”Ÿæˆ"""
    tags = tags or []
    frontmatter = f'''---
title: "{title}"
date: {date_iso}
draft: false
'''
    if tags:
        frontmatter += f"tags: {tags}\n"
    frontmatter += "---\n\n"
    return frontmatter


def generate_hugo_content(post: dict, media_dest_dir: Path, source_base: Path) -> tuple[str, str, list[str]]:
    """Hugo è¨˜äº‹ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ"""
    content = extract_post_content(post)
    attachments = extract_attachments(post)

    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆï¼ˆã‚³ãƒ”ãƒ¼å¯¾è±¡ï¼‰
    media_files = []

    # æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã«å¤‰æ›
    attachment_md = ""
    for att in attachments:
        if att['type'] == 'media' and att['uri']:
            uri = att['uri']
            # ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
            media_path = source_base / uri
            if media_path.exists():
                filename = os.path.basename(uri)
                media_files.append((str(media_path), filename))
                attachment_md += f"\n![{att.get('description', '')}]({filename})\n"
        elif att['type'] == 'link' and att['url']:
            link_text = att.get('name') or att['url']
            attachment_md += f"\n[{link_text}]({att['url']})\n"
        elif att['type'] == 'place' and att['name']:
            place_info = att['name']
            if att.get('address'):
                place_info += f" ({att['address']})"
            attachment_md += f"\nğŸ“ {place_info}\n"

    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ
    title = content[:100] if content else "FacebookæŠ•ç¨¿"
    title = sanitize_filename(title)
    if not title:
        title = "FacebookæŠ•ç¨¿"

    full_content = content + attachment_md

    return full_content, title, media_files


def load_facebook_posts(json_path: Path) -> list[dict]:
    """Facebook ã®æŠ•ç¨¿ JSON ã‚’èª­ã¿è¾¼ã‚€"""
    with open(json_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def convert_posts_to_hugo(
    input_json: Path,
    output_dir: Path,
    source_base: Path,
    max_posts: int = None
):
    """Facebook æŠ•ç¨¿ã‚’ Hugo è¨˜äº‹ã«å¤‰æ›"""
    posts = load_facebook_posts(input_json)

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    content_dir = output_dir / 'content' / 'posts'
    static_dir = output_dir / 'static' / 'images'
    content_dir.mkdir(parents=True, exist_ok=True)
    static_dir.mkdir(parents=True, exist_ok=True)

    converted_count = 0

    for i, post in enumerate(posts):
        if max_posts and i >= max_posts:
            break

        if 'timestamp' not in post:
            continue

        date_str, date_iso = convert_timestamp(post['timestamp'])

        # æŠ•ç¨¿ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
        content, title, media_files = generate_hugo_content(post, static_dir, source_base)

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒãªã„æŠ•ç¨¿ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if not content.strip() and not media_files:
            continue

        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        slug = sanitize_filename(title)[:30] if title else str(post['timestamp'])
        slug = re.sub(r'[^\w\-]', '-', slug)
        slug = re.sub(r'-+', '-', slug).strip('-')
        filename = f"{date_str}-{slug or post['timestamp']}.md"

        # è¨˜äº‹ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆPage Bundleå½¢å¼ï¼‰
        post_dir = content_dir / f"{date_str}-{slug or post['timestamp']}"
        post_dir.mkdir(parents=True, exist_ok=True)

        # ç”»åƒã‚’ã‚³ãƒ”ãƒ¼
        for src_path, dest_filename in media_files:
            dest_path = post_dir / dest_filename
            if os.path.exists(src_path) and not dest_path.exists():
                shutil.copy2(src_path, dest_path)

        # è¨˜äº‹ã‚’æ›¸ãå‡ºã—
        frontmatter = generate_hugo_frontmatter(date_iso, title)
        article_path = post_dir / 'index.md'
        with open(article_path, 'w', encoding='utf-8') as f:
            f.write(frontmatter)
            f.write(content)

        converted_count += 1
        if converted_count % 100 == 0:
            print(f"  {converted_count} ä»¶å¤‰æ›å®Œäº†...")

    return converted_count


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ãƒ‘ã‚¹ã®è¨­å®š
    base_dir = Path(__file__).parent
    source_base = base_dir / 'your_facebook_activity'
    input_json = source_base / 'posts' / 'your_posts__check_ins__photos_and_videos_1.json'
    output_dir = base_dir / 'hugo-blog'

    print("Facebook ãƒ‡ãƒ¼ã‚¿ã‚’ Hugo ãƒ–ãƒ­ã‚°è¨˜äº‹ã«å¤‰æ›ã—ã¾ã™...")
    print(f"å…¥åŠ›: {input_json}")
    print(f"å‡ºåŠ›: {output_dir}")

    if not input_json.exists():
        print(f"ã‚¨ãƒ©ãƒ¼: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_json}")
        return 1

    # å¤‰æ›ã‚’å®Ÿè¡Œ
    count = convert_posts_to_hugo(input_json, output_dir, source_base)

    print(f"\nå®Œäº†! {count} ä»¶ã®æŠ•ç¨¿ã‚’å¤‰æ›ã—ã¾ã—ãŸã€‚")
    print(f"å‡ºåŠ›å…ˆ: {output_dir}")

    return 0


if __name__ == '__main__':
    exit(main())
